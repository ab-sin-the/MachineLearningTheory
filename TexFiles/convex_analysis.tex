\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Convex Analysis}

\section{Convex Set and Convex Function}
In this section, we will review some concepts in convex analysis.

First, let us define some basic and useful notations.

\begin{definition}[\bfseries Gradient]
	Let $f: \R^n \to \R$ be a differentiable function and let $\xb \in \R^n$. We can defined the gradient of $f$ at $\xb$ as 
	
	\begin{equation}
	\nabla f(\xb) = (\frac{\partial f}{\partial \xb_1} (\xb), \cdots, \frac{\partial f}{\partial \xb_n} (\xb)).
	\end{equation}
\end{definition}

\begin{remark}
	Notice that here we define gradient as a row vector ($\R^{1\times n}$). However, for convenience, we may abuse this notation and use it as a column vector ($\R^{n\times 1}$) in the following contents.
\end{remark}
\begin{definition}[\bfseries Hessian]
	Let $f:\R^n \to \R$ be a twice differentiable function and let $\xb \in \R^n$. Then the Hessian of $f$ at $\xb$ is defined as
	
	\begin{equation}
	\nabla^2 f(\xb) = 
	\begin{pmatrix} 
	\frac{\partial ^2 f}{\partial x_1^2} (\xb) & \cdots  & \frac{\partial ^2 f}{\partial x_1 \partial x_n} (\xb) \\
	\vdots & \ddots & \vdots \\
	\frac{\partial ^2 f}{\partial x_n \partial x_1} (\xb) & \cdots  & \frac{\partial ^2 f}{ \partial x_n^2} (\xb).
	\end{pmatrix}
	\end{equation}
	
\end{definition}

Notice that if $f$ is a twice differentiable function and $\xb \in \R^n$, then $\nabla^2 f(\xb)$ is a symmetric matrix since for all $i, j \in [1, n]$, it has

\begin{equation*}
	\frac{\partial ^2 f}{\partial x_i \partial x_j} (\xb) = \frac{\partial ^2 f}{\partial x_j \partial x_i} (\xb).
\end{equation*}

Next, we will talk about convexity, its definition and several useful facts.

\begin{definition}[\bfseries Convex Set]
	Let $\K \subset \R^n$, a set $\K$ is called a convex set if  $\ \forall \xb,\yb \in \K$ and $\forall t \in [0,1]$, we have
	
	\begin{equation}
		t\xb + (1-t)\yb \in \K.
	\end{equation}
	
	The meaning of this definition is that for any two arbitrary points in the set $\K$, the straight line between these two points are all contained in $\K$.
\end{definition}

\begin{definition}[\bfseries Convex Function]
	Let $\K \subset \R^n$ be a convex set and $f: \K \to \R^n$ be a differentiable function. Then function $f$ is convex if $\forall \xb,\yb \in \K$ and $\forall t\in [0, 1]$, we have that
	
	\begin{equation}
	f((1-t)\xb + t \yb) \leq (1-t) f(\xb)+ t f(\yb). \label{convex_f_1}
	\end{equation}
\end{definition}


\begin{claim}
	Let $\K \subset \R^n$ be a convex set and $f: \K \to \R^n$ be a differentiable function. Then function $f$ is convex if and only if $\forall \xb,\yb \in \K$, we have that
	
	\begin{equation}
		f(\xb) \geq f(\yb) + \langle \nabla f(\yb) , \xb - \yb \rangle. \label{convex_f_2}
	\end{equation}
\end{claim}

An intuition behind this claim is that $f$ is convex if and only if the first order approximation of $f$ at any point $\yb$ is not larger than the function itself.
\begin{claim}
	Let $\K \subset \R^n$ be a convex set and $f: \K \to \R$ be a twice differentiable function, then $f$ is convex if and only if  $\forall \xb \in \K$, 
	
	\begin{equation}
		\nabla^2 f(\xb) \succeq 0. \label{convex_f_3}
	\end{equation}
\end{claim}

\begin{remark}
	 It can be seen that above three formulas \eqref{convex_f_1}, \eqref{convex_f_2} and \eqref{convex_f_3} are equivalent. However, in reality, if $f$ is twice differentiable, using \eqref{convex_f_3} is much simpler than using \eqref{convex_f_1} or \eqref{convex_f_2}, because it only contains one variable in $\K$ and we only need to determine if $\nabla^2f(\xb)$ is a PSD matrix or not.
	 
	   In facts, many functions satisfy this condition. Some examples are $f(\xb) = \xb^T \xb$, $f(\xb) = 1$, etc.
\end{remark}


\begin{proposition} $\quad$ 
	\begin{enumerate}
		\item If $f$ is convex and $g$ is convex, then $f + g$ is convex.
		\item If $f$ is convex, then $\forall \alpha \geq 0 $, $\alpha f$ is convex.
		\item If $f$ is convex and $g$ is convex, then $\max\{f,g\}$ is convex.
		\item If $g(\xb, \yb)$ is jointly convex in $\xb$, $\yb$, then $f(\xb) = \inf\limits_{\yb} g(\xb, \yb)$ is convex.
	\end{enumerate}
\end{proposition}

\begin{definition}[\bfseries Concave Function]
	Let $\K \subset  \R^n $ be a convex set and $f:\K \to \R$, then $f$ is concave is $-f$ is convex.
\end{definition}

We can see that one example of concave function is $log$ function.


\begin{definition}[\bfseries Lipschitz]
	Let $||\cdot||$ be a norm, $c \geq 0$ and $f:\R^n \to \R$. Then $f$ is called $c$-Lipschitz with respect to $||\cdot||$ if $\forall \xb, \yb \in \R^n$, 
	
	\begin{equation}
		|f(\xb) - f(\yb)| \leq c||\xb - \yb||.
	\end{equation}
	
	Intuitively, it means that the difference between function value is smaller than the difference between the norm of difference between two points times some constant c.
\end{definition}

\begin{claim}
	If $f$ is a differentiable function, then $f$ is $c$-$Lipschitz$ with respect to $||\cdot||$ if and only if $\forall \xb \in \R^n$,
		\begin{equation}
	||\nabla f(\xb)||_* \leq c
	\end{equation}
	where $||\cdot||_*$ is the dual norm of $||\cdot||$.
\end{claim}
\begin{proof}
	We first prove the 'if' part.
	
	Let $||\cdot||$ be a norm on $\R^n$ and $c \geq 0$, let $||\nabla f(\xb)||_* \leq c \ $, $\forall \xb \in \R^n$. Then, by the mean value theorem, $\forall \xb, \yb \in\R^n$, there exists $t\in [0,1]$ such that 
	\begin{equation*}
		f(\xb) = f(\yb) + \langle \nabla f((1-t) \xb + t \yb) , \xb - \yb \rangle.
	\end{equation*}
	
	Then, by H\"older's inequality, 
	
	\begin{equation*}
		\begin{aligned}
				|f(\xb) - f(\yb)| & = |\langle \nabla f((1-t) \xb + t \yb) , \xb - \yb \rangle| \\
								  & \leq ||\nabla f((1-t) \xb + t \yb)||_* ||\xb - \yb|| \\
								  & \leq c ||\xb - \yb||.
		\end{aligned}
	\end{equation*}
	That is, f is $c$-$Lipschitz$ with respect to $||\cdot||$.
	
	We will next prove the 'only if' part.
	
	Suppose that $f$ is $c$-$Lipschitz$ with respect to $||\cdot||$ and let $\xb, \yb \in \R^n$, then we have 
	
	\begin{equation*}
		\begin{aligned}
			\langle \nabla f(\xb) , \yb \rangle &= \lim\limits_{h \to 0} \frac{f(\xb + h\yb) - f(\yb)}{h} \\
												&\leq \lim\limits_{h \to 0} \frac{c||\xb + h\yb - \xb||}{h} \\
												& = c \lim\limits_{h \to 0} \frac{h||\yb||}{h} \\
												& = c||\yb||.
		\end{aligned}
	\end{equation*}
	
	Notice that $\langle \nabla f(\xb) , \yb \rangle$ is the directional derivative of $f$ at $\xb$ in the direction of $\yb$. Then,
	
	\begin{equation*}
		||\nabla f(\xb)||_* = \sup\limits_{||\yb||\leq 1} \langle \nabla f(\xb) , \yb \rangle \leq \sup\limits_{||\yb||\leq 1}  c||\yb|| = c.
	\end{equation*}
	
	Therefore, $\forall \xb \in \R^n$, $||\nabla f(\xb)|| \leq c$.
\end{proof}


\begin{theorem}[\bfseries Jensen's Inequality]
	Let $\K \in \R^n$ be a convex set, $X$ be a random variable on $\K$ and $f:\K \to \R$ be a convex function, then
	\begin{equation}
		f(\EE[X]) \leq \EE[f(X)]
	\end{equation}
	
	where $\EE$ denotes the expectation of a random variable.
\end{theorem}

\begin{theorem}[\bfseries Young's Inequality]
	Let $p, q > 0$ and satisfy that $\frac{1}{p} + \frac{1}{q} = 1$. Then $\forall a, b > 0$,
	\begin{equation}
		ab \leq \frac{a^p}{p} + \frac{b^q}{q}.
	\end{equation}
\end{theorem}

\begin{proof}
	Since $-\log$ is a convex function, then $\forall a, b > 0$,
	
	\begin{equation*}
		\begin{aligned}
			\log(ab) & =\log(a) + \log(b) \\
					 & = \frac{p}{p} \log (a) + \frac{q}{q}\log (b)\\
					 & = \frac{1}{p} \log(a^p) + \frac{1}{q} \log(b^q) \\
					 & \leq \log (\frac{a^p}{p} + \frac{b^q}{q} ).
		\end{aligned}
	\end{equation*}
	
	Then apply $\exp$ to both side and since $\exp$ function is monotonically increasing, we can have that 
	
	\begin{equation*}
	ab \leq \frac{a^p}{p} + \frac{b^q}{q}.
	\end{equation*}
\end{proof}


\begin{definition}[\bfseries Strongly Convex]
	Let $\K$ be a convex set, $f: \K \to \R$ be a differentiable function and $\alpha > 0$. Then $f$ is $\alpha$-strongly convex with respect to $||\cdot||$ if $\forall \xb, \yb \in \K$, 
	\begin{equation}
		f(\xb) \geq f(\yb) + \langle \nabla f(\yb), \xb - \yb \rangle + \frac{\alpha}{2} ||\xb - \yb||^2.
	\end{equation}
\end{definition}

Notice that if $f$ is strongly convex, then it must be a convex function. In addition, a strongly convex function grows at least quadratically. 

\begin{claim}\label{strongly-convex-lemma}
	Let $f$: dom($f$) $\to \R$ be a twice differentiable function, then $f$ is $\alpha$-strongly convex if and only if  $\forall \xb \in \textmd{dom}(f)$,
		\begin{equation*}
			\nabla^2 f(\xb) - \alpha I \succeq 0.
		\end{equation*}
\end{claim}

\begin{definition}[\bfseries Smooth]
	Let $f$: dom($f$) $\to \R$ be a differentiable function, let $\alpha >0$, then $f$ is $\alpha$-smooth if $\forall \xb, \yb \in $ dom($f$), 
	\begin{equation*}
		f(\xb) \leq f(\yb) + \langle \nabla f(\yb), \xb - \yb \rangle + \frac{\alpha}{2} ||\xb - \yb||^2.
	\end{equation*}
\end{definition}

Notice that in contrast to strongly convex, a smooth function grows \textbf{at most} quadratically.

\begin{claim}\label{smooth-lemma}
	We have a similar claim for smooth function as strongly convex function. That is, let $f$: dom($f$) $\to \R$ be a twice differentiable function, then $f$ is $\alpha$-smooth if and only if $\forall \xb \in \textmd{dom}(f)$, we have
	\begin{equation*}
		\nabla^2 f(\xb) - \alpha I \preceq 0.
	\end{equation*}
\end{claim}

Notice that although smooth seems to be contrary to strongly convex, a function $f$ can be both $\alpha$-strongly convex and $\beta$-smooth for some $\alpha$ and $\beta$. We will give an example next.

\begin{example}
	Let $M \in \R^{n\times n}$ be a positive definite matrix and let $f(\xb) = \frac{1}{2} \xb^T M \xb$. Denote $\lambda_{min}$ as the smallest eigenvalue of $M$ and $\lambda_{max}$ as the largest eigenvalue of $M$. Then we can prove easily that $f$ is $\lambda_{min}$-strongly convex and $\lambda_{max}$-smooth.
\end{example}

\begin{proof}
	
	First, by taking derivative, it can be seen that $\nabla f(\xb) = \xb^T M$. Then, take the second derivative and we can get $\nabla^2 f(\xb) = M$. By Claim \ref{smooth-lemma} and Claim \ref{strongly-convex-lemma}, we can show that $f$ is $\lambda_{min}$-strongly convex and $\lambda_{max}$-smooth.
\end{proof}

\section{Bregman Divergence}
In this section, we will define Bregman Divergence and see its relation with convexity analysis.

\begin{definition}
	Given a function $f$: $\K \to \R$, we can define its Bregman Divergence as
	\begin{equation}
		D_f(\xb, \yb) := f(\xb) - f(\yb) - \langle \nabla f(\yb), \xb - \yb \rangle.
	\end{equation}
\end{definition}

\begin{example}
		Let $f(\xb) = \frac{1}{2} ||\xb||_2^2$, then
		\begin{equation*}
		D_f(\xb, \yb) = \frac{1}{2} ||\xb - \yb||_2^2.
		\end{equation*}
		Notice that only in this situation is Bregman divergence a quadratic function.
\end{example}

\begin{example}
	Let $\Delta^n = \{\p \in \R^n| \sum_{i=1}^n p_i = 1, p_i \geq 0\}$ be the simplex with dimension n. Let $f(\p) = \sum_{i=1}^n p_i\log p_i$ be the entropy function and suppose $f(0) = 0$. Then
	\begin{equation*}
	D_f(\p,\qb) = \sum\limits_{i=1}^n p_i\log \frac{p_i}{q_i} = \textmd{KL}(\pb || \qb),
	\end{equation*}
	
	where KL is Kullback-Leibler divergence.
\end{example}

We will next talk about some facts about Bregman divergence.

\begin{fact} $ $
	\begin{enumerate}
		\item If $f$ is convex, then $D_f (\xb, \yb) = 0$ if and only if $\xb=\yb$.
		\item $f$ is $\mu$-strongly convex if and only if $D_f(\xb, \yb) \geq \frac{\mu}{2} ||\xb-\yb||^2$.
		\item $f$ is $\beta$-smooth if and only if $D_f(\xb, \yb) \leq \frac{\beta}{2} ||\xb - \yb||^2$
		\item If $D_f(\xb, \yb) = D_f(\yb, \xb)$, then $f$ is quadratic.
		\item Pinsker's Inequality: $KL(\p|| \qb) \geq \frac{1}{2} ||\p - \qb||_1^2$. It means that entropy function is $1$-strongly convex in $||\cdot||_1$.
	\end{enumerate}
\end{fact}

\begin{definition}[\bfseries Fenchel Dual Conjugate]
	Let $f$ be convex, the Fenchel dual conjugate of $f$ is defined as
	\begin{equation}
		f^*(\theta) = \sup\limits_{\xb\in \textmd{dom}(f)} \left[ \langle \xb, \theta \rangle - f(\xb)\right].
	\end{equation}
\end{definition}

\begin{claim}
	$f^*(\theta)$ is convex.
\end{claim}

\begin{proof}
	It can be seen that $ g_\xb(\theta) = \langle \xb, \theta \rangle - f(\xb)$ is a linear function. Linear function is convex. In addition, $f^*(\theta) = \sup\limits_{\xb\in \textmd{dom}(f)} g_\xb(\theta)$ is the supremum of convex function, which means that $f$ is also convex.  
\end{proof}

\begin{example}
	If $f(\xb) = \frac{1}{2} ||\xb||_2^2$, then $f^*(\theta) = \frac{1}{2} ||\theta||_2^2$.
\end{example}

\begin{example}
	If $f(\xb) = \frac{1}{2}\x^T M \x$ and $M$ is a positive definite matrix, 
	
	\begin{equation*}
	f^*(\theta) = \sup\limits_{\xb\in \textmd{dom}(f)} \left[ \langle \xb, \theta \rangle -  \frac{1}{2}\x^T M \x \right].
	\end{equation*}
	
	Let $g(\x) =\langle \xb, \theta \rangle -  \frac{1}{2}\x^T M \x $. Take the derivative over $\x$ and we can get that
	\begin{equation*}
	\begin{aligned}
		&	\nabla_\x g(\x) = 0\\
	\Leftrightarrow\ & \theta - M \x = 0\\
	\Leftrightarrow\ & \x = M^{-1} \theta. 
	\end{aligned}
	\end{equation*}
	
	Substitute the value of $\x$ into $f^*(\theta)$ and we can get that 
	\begin{equation*}
		\begin{aligned}
				f^*(\theta) &= \theta^T M^{-1}\theta - \frac{1}{2} (M^{-1}\theta)^T M M^{-1}\theta\\
							&= \frac{1}{2} \theta^T M^{-1} \theta.
		\end{aligned}
	\end{equation*}
\end{example}

\begin{example}
	Let $f(x) = \frac{1}{p} ||\x||_p^p = \frac{1}{p} \sum\limits_{i=1}^n x_i^p$, then
	\begin{equation*}
		f^*(\theta) = \frac{1}{q} ||\theta||_q^q,
	\end{equation*}
	where $\frac{1}{p} + \frac{1}{q} = 1$.
\end{example}

There is one famous inequality for Fenchel dual conjugate called Fenchel-Yang Inequality.

\begin{theorem}[\bfseries Fenchel-Yang Inequality]
	$\forall \x \in \textmd{dom}(f)$ and $\forall \theta \in \textmd{dom}(f^*)$, it holds that 
	\begin{equation}
	f(\x) + f^*(\theta) \geq \langle \x, \theta \rangle.
	\end{equation}
\end{theorem}

\begin{proof}
	By the definition of Fenchel dual conjugate, $\forall \x$ and $\theta$,
	\begin{equation*}
	\begin{aligned}
		f^*(\theta) + f(\x) & = \sup_\y \left[ \langle \y, \theta \rangle - f(\y)\right] +f(\x) \\
		&\geq  \langle \x, \theta \rangle - f(\x) + f(\x) \\
		& = \langle \x, \theta \rangle.
	\end{aligned}
	\end{equation*}
\end{proof}

\begin{corollary}
	By the inequality above, we can get directly that
	\begin{equation*}
		\frac{1}{p} ||\x||_p^p + \frac{1}{q} ||\theta||_q^q \geq \langle \x, \theta\rangle. 
	\end{equation*}
\end{corollary}

I will next discuss some facts about Fenchel dual.
\begin{fact}$ $
	\begin{enumerate}
		\item If $f$ is closed, then $(f^*)^* = f$.
		\item $\nabla f (\nabla f^*(\theta)) = \theta$ and $\nabla f^*(\nabla f(\x)) =\x$.
		\item If $f$ is differentiable, then $D_f(\x,\y) = D_{f^*}(\nabla f(\y), \nabla f(\x))$.
		\item If $f$ is $\mu$-strongly convex with respect to $||\cdot||$, then $f^*$ is $\frac{1}{\mu}$-smooth with respect to $||\cdot||_*$, where $||\cdot||_*$ is dual to $||\cdot||$.
	\end{enumerate}
\end{fact}
\end{document}